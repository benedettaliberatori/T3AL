<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Test-Time Zero-Shot Temporal Action Localization. Accepted to CVPR 2024.">
  <meta property="og:title" content="Test-Time Zero-Shot Temporal Action Localization" />
  <meta property="og:description" content="Learn more about T3AL, accepted to CVP3 2024." />
  <meta property="og:url" content="https://benedettaliberatori.github.io/T3AL/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/method-v7-1.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Test-Time Zero-Shot Temporal Action Localization.">
  <meta name="twitter:description" content="Test-Time Zero-Shot Temporal Action Localization. Accepted to CVPR 2024.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/method-v7-1.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="T3AL, temporal action localization, zero-shot, test-time, CVPR 2024">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Test-Time Zero-Shot Temporal Action Localization</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>

<body>


  <style>
    .author-block {
      margin-right: 10px;
      /* Adjust the value as per your preference */
    }
  </style>

  <style>
    /* Custom CSS for tooltip */
    .custom-tooltip .tooltip-inner {
      background-color: #f1f1f1;
      color: #333333;
    }

    .custom-tooltip .tooltip.bs-tooltip-top .arrow::before {
      border-top-color: #f1f1f1;
    }
  </style>


  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><b style="font-size: 64px;">
              T3AL
              <img src="static/images/duck.jpg" alt="Icon" style="width: 100px; vertical-align: middle;">
            </b></h1>
            <h1 class="title is-1 publication-title publication-title-single-line">Test-Time Zero-Shot Temporal Action Localization</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Benedetta Liberatori</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://alessandroconti.me/" target="_blank">Alessandro Conti</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://paolorota.github.io/" target="_blank">Paolo Rota</a><sup>1</sup>,
              <span class="author-block">
                <a href="https://www.yimingwang.it/" target="_blank">Yiming Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://elisaricci.eu/" target="_blank">Elisa Ricci</a><sup>1,2</sup>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block" style="margin-left: 10px;"></span><sup>1</sup></span> University of Trento
              <span class="author-block" style="margin-left: 10px;"></span><sup>2</sup></span> Fondazione Bruno Kessler
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2404.05426v1.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Github link -->
            <span class="link-block">
              <a href="https://github.com/benedettaliberatori/T3AL" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>

            <!-- ArXiv abstract Link -->
            <span class="link-block">
              <a href="https://arxiv.org/abs/2404.05426v1" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>

          </div>
        </div>
      </div>
    </div>
  </section>

<!--
  <div class="image-column">
    <div class="image-container">
        <img src="static/images/teaser-v5a-1.png" alt="Image 1" width="40%">
        <div class="caption">(a) Previous approaches</div>
    </div>
    <div class="image-container">
        <img src="static/images/teaser-v5b-1.png" alt="Image 2" width="40%">
        <div class="caption">(b) Our proposal</div>
    </div>
</div>
-->


    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
              Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and 
              locate actions in untrimmed videos unseen during training. Existing 
              ZS-TAL methods involve fine-tuning a model on a large amount of annotated 
              training data. While effective, training-based ZS-TAL approaches assume 
              the availability of labeled data for supervised learning, which can be 
              impractical in some applications. Furthermore, the training process 
              naturally induces a domain bias into the learned model, which may 
              adversely affect the model's generalization ability to arbitrary videos. 
              These considerations prompt us to approach the ZS-TAL problem from a 
              radically novel perspective, relaxing the requirement for training data. 
              To this aim, we introduce a novel method that performs Test-Time adaptation 
              for Temporal Action Localization (<em>T3AL</em>). In a nutshell, <em>T3AL</em>
              adapts a pre-trained Vision and Language Model (VLM) at inference time on 
              a sample basis. <em>T3AL</em> operates in three steps. First, a 
              video-level pseudo-label of the action category is computed by 
              aggregating information from the entire video. Then, action localization
              is performed adopting a novel procedure inspired by self-supervised learning. 
              Finally, frame-level textual descriptions extracted with a state-of-the-art 
              captioning model are employed for refining the action region proposals.
              We validate the effectiveness of by conducting experiments on the THUMOS14 
              and the ActivityNet-v1.3 datasets. Our results demonstrate that significantly 
              outperforms zero-shot baselines based on state-of-the-art VLMs, confirming 
              the benefit of a test-time adaptation approach.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->


    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <h2 class="title is-3">Method Overview</h2>
          <img src="static/images/method-v7-1.png" alt="Banner Image" height="100%">
          <h2 class="subtitle has-text-justified" style="line-height: 1.6;">
            <b>Illustration of our proposed framework.</b> 
            <em>T3AL</em> addresses the task of ZS-TAL by only 
            learning at test-time on unlabelled data.
            We first compare the average visual frames with the 
            textual class names to identify the video pseudo-label  <img src="static/images/icon_star-1.png" alt="Icon" style="width: 30px; vertical-align: middle;">. 
            We then refine the visual frames - video pseudo-label 
            scores with self-supervision. Last, we exploit the decoder 
            of a captioning model to generate captions and perform 
            text-guided region suppression. We only <img src="static/images/icon_fire-1.png" alt="Icon" style="width: 25px; vertical-align: middle;"> fine-tune the 
            vision and language projectors, while keeping the encoders <img src="static/images/icon_ice-1.png" alt="Icon" style="width: 25px; vertical-align: middle;">
            frozen. Once the prediction is obtained, the optimized parameters
            are re-initialized to the ones of the VL pre-trained model.
          </h2>
        </div>
      </div>
    </section>




  <!-- Video description-->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="hero-body">

        <h2 class="title is-3">Qualitative Results</h2>
        <div class="content has-text-justified" style="line-height: 1.6;">
          <p>
            In each of the following illustrations we show a video from THUMOS14 and
            the prediction of our proposed method <span class="tex">\(T^3AL\)</span>.
            The visual representation includes ground truth and predicted classes,
            as well as the similarity with the pseudo-label. In the similarity plot,
            temporal ground truth action intervals are highlighted in green,
            predicted action proposals in blue, and <strong>overlapping
              areas</strong> are indicated by <strong>parallel diagonal lines</strong>. The red slider
            visually represents the progression of time within the video.
            Videos are better seen in full screen.
        </div>
      </div>

    </div>
  </section>
  <!-- End Video description -->



  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_validation_0000852.mp4" type="video/mp4">
        </video>

        The predicted class correspods to the ground truth class. The model
        discriminates between regions in the video with the penalty
        kick and regions with other actions related to the game of soccer,
        such as players running or walking on the field and soccer passes.
        The three false positive regions predicted contain the preparation
        for the penalty kick and thus are very similar visually to the
        ground truth action.


      </div>
    </div>
  </section>
  <!-- End teaser video -->


  <!-- Teaser video-->
  <section class="hero teaser is-light">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_test_0000673.mp4" type="video/mp4">
        </video>
          The predicted class corresponds to the ground truth class and the predicted
          regions are aligned with the ground truth ones most of the time. There are
          two false positive regions predicted, containing a man jumping on the diving board,
          and one prediction significantly bigger than the corresponding ground truth region,
          depicting a man doing an handstand on the diving board. In both cases, we can see
          why the model wrongly assigns relatively high scores to these predictions, as the scenes
          share visual similarities with the ground truth.
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Teaser video-->
  <section class="hero teaser ">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_validation_0000949.mp4" type="video/mp4">
        </video>
          In this example, the ground truth class is misclassified, the model
          predict the action of throwing an hammer instead of a discus. 
          Moreover, the model predicts two false positive regions, confused by the
          fact that the scene is similar to the one of the ground truth action.
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Teaser video-->
  <section class="hero teaser is-light">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_validation_0000940.mp4" type="video/mp4">
        </video>
          In this example the predicted class is correct while
          there is less overlap between the ground truth and the predicted
          regions. Upon closer inspection of the video, it becomes apparent
          that the model detects the action in frames where a
          player is practicing a tennis swing with an elastic band.
          These frames have not been included in the ground truth regions
          by annotators, yet it can be argued that they contain elements of the action class in question.
      </div>
    </div>
  </section>
  <!-- End teaser video -->


  <!-- Teaser video-->
  <section class="hero teaser ">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_test_0000946.mp4" type="video/mp4">
        </video>
          The ground truth class is classified and the two occurrencies
          of it are detected. The model predicts a false positive region and wrongly 
          considers the aftermath of the action as part of the action itself.
      </div>
    </div>
  </section>
  <!-- End teaser video -->


  <!-- Teaser video-->
  <section class="hero teaser is-light">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_validation_0000059.mp4" type="video/mp4">
        </video>
          In this example, the predicted class corresponds to the ground truth class.
          The model predicts a region proposal for each one of the occurrences of the action,
          yet the predicted regions are longer than the ground truth ones. This can be
          attributed to the fact that the class name encompasses
          a wide range of actions associates with the billiard game.
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Teaser video-->
  <section class="hero teaser ">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_validation_0000265.mp4" type="video/mp4">
        </video>
          The ground truth class is classified and for the most of the video
          there is overlap between the ground truth and the predicted regions.
          The model misses a big part of the last occurrence of the action. Upon closer
          inspection of the video, it can be seen that it predicts a low similarity with the pseudo-label as the
          scene contains a man that is running.
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Teaser video-->
  <section class="hero teaser is-light">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_test_0000026.mp4" type="video/mp4">
        </video>

        The predicted class corresponds to the ground truth one. The model is able to detect
        large areas where the action takes place, yet it wrongly aggregates multiple regions into five single
        ones. This can be attributed to the fact that there is a subtle difference between a tennis
        swing and the preparation or the aftermath of it.


      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Teaser video-->
  <section class="hero teaser ">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_validation_0000156.mp4" type="video/mp4">
        </video>
          In this example, the action is classified but the similarity
          with it is high over the whole video and the model predicts many false
          positive regions. This can be attributed to the fact that there are
          different moments in the video when the action is partially performed.
          For instance, there are instances when only the clean is performed without
          the full clean and jerk, or when the individual is unable to lift the barbell entirely.
      </div>
    </div>
  </section>
  <!-- End teaser video -->





  <!-- Teaser video-->
  <section class="hero teaser is-light">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="videos/video_validation_0000986.mp4" type="video/mp4">
        </video>
          The action is classified, and all the ground truth instances are detected.
          All predicted regions are longer than the ground truth ones because the model considers the aftermath
          of the action as part of the action itself.
      </div>
    </div>
  </section>
  <!-- End teaser video -->





  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.10.2/umd/popper.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.min.js"></script>

  <script>
    $(function () {
      $('[data-toggle="tooltip"]').tooltip();
    });
  </script>


  <footer class="footer" style="font-size: smaller;">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>





  <!-- Statcounter tracking code -->
  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
  <!-- End of Statcounter Code -->

</body>

</html>